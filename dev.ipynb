{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model pieces: 32000\n",
      "\tadded <my-cool-function> ...\n",
      "\tadded <endtitle> ...\n",
      "\tadded <name> ...\n",
      "\tadded <url> ...\n",
      "\tadded <digit> ...\n",
      "\tadded <email> ...\n",
      "\tadded <loc> ...\n",
      "\tadded <greeting> ...\n",
      "\tadded <salutation> ...\n",
      "New model pieces: 32009\n",
      "['▁This', '▁is', '▁a', '▁test', '▁calling', '▁', '<digit>', '(', 'l', 'kn', 'kn', 'x', ')', '▁see', '▁what', '▁happens', '/']\n",
      "['▁This', '▁is', '▁a', '▁test', '▁calling', '▁', '<my-cool-function>', '(', 'l', 'kn', 'kn', 'x', ')', '▁see', '▁what', '▁happens', '/']\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sentencepiece import sentencepiece_model_pb2 as model\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "def augment_tokenizer(in_tok_path, fun_names, out_tok_path):\n",
    "\n",
    "    mp = model.ModelProto()\n",
    "    mp.ParseFromString(open(in_tok_path, 'rb').read())\n",
    "\n",
    "    # mp.ParseFromString(open(model_file, 'rb').read())\n",
    "\n",
    "    print(f'Original model pieces: {len(mp.pieces)}')\n",
    "\n",
    "    for i, sym in enumerate(fun_names, 1):\n",
    "        new_sym = mp.SentencePiece()\n",
    "        new_sym.piece = sym \n",
    "        new_sym.score = 0.0 # default score for USER_DEFINED\n",
    "        new_sym.type = 4 # type value for USER_DEFINED\n",
    "        mp.pieces.insert(2+i, new_sym) # position after default control symbols (\"<unk>\", \"<s>\", \"</s>\")\n",
    "        print(f'\\tadded {new_sym.piece} ...')\n",
    "\n",
    "    print(f'New model pieces: {len(mp.pieces)}')\n",
    "    \n",
    "    with open(out_tok_path, 'wb') as f:\n",
    "        f.write(mp.SerializeToString())\n",
    "    \n",
    "in_tok_path = '/home/karypisg/romer333/projects/LLM-tools/models/llama_checkpoints/tokenizer.model'\n",
    "out_tok_path = '/home/karypisg/romer333/projects/LLM-tools/ToolkenGPT/augmented_llama_tokenizer.model'\n",
    "fun_names = ['<my-cool-function>', '<endtitle>', '<name>', '<url>', '<digit>', '<email>', '<loc>', '<greeting>', '<salutation>']\n",
    "\n",
    "augment_tokenizer(in_tok_path, fun_names, out_tok_path)\n",
    "\n",
    "# Test\n",
    "sp_model = SentencePieceProcessor(model_file=out_tok_path)\n",
    "print(sp_model.encode_as_pieces(\"This is a test calling <digit>(lknknx) see what happens/\"))\n",
    "print(sp_model.encode_as_pieces(\"This is a test calling <my-cool-function>(lknknx) see what happens/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat for cross-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat kamel data in the same format as funcqa without functions\n",
    "import json\n",
    "kamel_filepath = './data/kamel/test_first_20.json'\n",
    "funcqa_filepath = './data/funcqa/funcqa_oh.json'\n",
    "\n",
    "with open(kamel_filepath, 'rb') as fp:\n",
    "    kamel_data = json.load(fp)\n",
    "    \n",
    "with open(funcqa_filepath, 'rb') as fp:\n",
    "    funcqa_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A car depreciates by 11% each year. In 8 years, what is the value of the car of its original price? (in decimal form)',\n",
       " 'answer': 0.3936,\n",
       " 'func': '<power>(0.89, 8)=0.3936'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcqa_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many floors above the ground has Claridge Icon?',\n",
       " 'answer': [{'rdf': None, 'alternative': ['45'], 'chosen': '45'}],\n",
       " 'api': 'P1101'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kamel_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in kamel_data:\n",
    "    record['answer'] = record['answer'][0]['chosen']\n",
    "    record['func'] = ''\n",
    "    del record['api']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/kamel/reformat_test_first_20.json', 'w') as fp:\n",
    "    json.dump(kamel_data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [29871, 316]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import sentencepiece as spm\n",
    "# vocab_file = 'sentence.bpe.model'\n",
    "vocab_file = '/home/karypisg/romer333/projects/LLM-tools/models/llama_checkpoints/tokenizer.model'\n",
    "test_string = '▁de'\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(vocab_file)\n",
    "new_vocab = ['<s>', '▁de', '-']\n",
    "\n",
    "\n",
    "print('encoded:', sp.encode_as_ids(test_string)) # 7\n",
    "original_vocab = [sp.IdToPiece(id) for id in range(0, sp.GetPieceSize())]\n",
    "sp.set_vocabulary(new_vocab)\n",
    "v2 = [sp.IdToPiece(id) for id in range(0, sp.GetPieceSize())]\n",
    "\n",
    "# with open(model_path, 'rb') as fp:\n",
    "#     data = fp.read()\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_new_model_path = \"./my_extended_tokenizer.model\"\n",
    "\n",
    "# sp_model.ResetVocabulary\n",
    "# sp_model.SetVocabulary([sp.IdToPiece(id) for id in range(0, sp.GetPieceSize())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁test',\n",
       " '▁calling',\n",
       " '▁',\n",
       " '<my-cool-function>',\n",
       " '(',\n",
       " 'l',\n",
       " 'kn',\n",
       " 'kn',\n",
       " 'x',\n",
       " ')',\n",
       " '▁see',\n",
       " '▁what',\n",
       " '▁happens',\n",
       " '/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [29871, 316]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32000, 32000, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_vocab), len(v2), print('encoded:', sp.encode_as_ids(test_string)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499723"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentencepiece import sentencepiece_model_pb2 as model\n",
    "m = model.ModelProto()\n",
    "m.ParseFromString(open(vocab_file, 'rb').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piece: \"<unk>\"\n",
       "score: 0.0\n",
       "type: UNKNOWN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pieces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m  \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "  \u001b[0;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Inserts the item at the specified position by copying.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnew_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/playground-augmented-llm/lib/python3.11/site-packages/google/protobuf/internal/containers.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?? m.pieces.insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size: 32000\n",
      "Extended vocab size: 32001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sentencepiece import sentencepiece_model_pb2 as model\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "def new_piece_by_deepcopy(original_piece,token:str,score:float):\n",
    "    '''\n",
    "    Args:\n",
    "        original_piece:(SentencePiece) the target of deepcopy\n",
    "        piece:(str) token\n",
    "        score:(float) priority of encoding to this token (see spm.vocab). \n",
    "        piece_type:(int) 1:normal, 2:<unk>, 3:control, 4:user defined, 5:unused. \n",
    "        \n",
    "    Return:\n",
    "        a SentencePiece with given piece, score and piece_type\n",
    "    '''\n",
    "    new_p=deepcopy(original_piece)# not a good way, but it does work.\n",
    "    new_p.piece=token\n",
    "    new_p.score=score\n",
    "    return new_p\n",
    "\n",
    "# m.pieces.insert(0, new_piece_by_deepcopy(m.pieces[0],\"<my_cool_tool>\",0,m.pieces[0].type))\n",
    "\n",
    "vocab_file = '/home/karypisg/romer333/projects/LLM-tools/models/llama_checkpoints/tokenizer.model'\n",
    "\n",
    "m = model.ModelProto()\n",
    "m.ParseFromString(open(vocab_file, 'rb').read())\n",
    "\n",
    "print(\"Original vocab size:\",len(m.pieces))\n",
    "\n",
    "new_piece = new_piece_by_deepcopy(m.pieces[1800],\"<my_cool_tool>\",1e50)\n",
    "m.pieces.insert(0, new_piece)\n",
    "# m.\n",
    "\n",
    "print(\"Extended vocab size:\",len(m.pieces))\n",
    "# m.pieces[32000]\n",
    "\n",
    "with open(\"./my_extended_tokenizer.model\",\"wb\") as f:\n",
    "    f.write(m.SerializeToString())\n",
    "\n",
    "my_new_model_path = \"./my_extended_tokenizer.model\"\n",
    "sp_model = SentencePieceProcessor(model_file=my_new_model_path)\n",
    "# sp_model.ResetVocabulary\n",
    "sp_model.SetVocabulary([sp.IdToPiece(id) for id in range(0, sp.GetPieceSize())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piece: \"<0x10>\"\n",
       "score: 0.0\n",
       "type: BYTE"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pieces[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "']{'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model.IdToPiece(3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁<', 'my', '_', 'co', 'ol', '_', 'tool', '>']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model.encode_as_pieces(sp_model.IdToPiece(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m.pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piece: \"<my_cool_tool>\"\n",
       "score: 0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pieces[32000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./my_extended_tokenizer.model\",\"wb\") as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "my_new_model_path = \"./my_extended_tokenizer.model\"\n",
    "sp_model = SentencePieceProcessor(model_file=my_new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<my_cool_tool>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model.IdToPiece(32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sentencepiece._sentencepiece' has no attribute 'SentencePiece'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePiece\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sentencepiece._sentencepiece' has no attribute 'SentencePiece'"
     ]
    }
   ],
   "source": [
    "m.SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499723"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentencepiece import sentencepiece_model_pb2\n",
    "\n",
    "m = sentencepiece_model_pb2.ModelProto()\n",
    "m.ParseFromString(data)\n",
    "# model = sentencepiece_model_pb2(model_file=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentencepiece_model_pb2.SentencePiece"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m.pieces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sentencepiece.sentencepiece_model_pb2' has no attribute 'SentencePiece'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msentencepiece_model_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePiece\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sentencepiece.sentencepiece_model_pb2' has no attribute 'SentencePiece'"
     ]
    }
   ],
   "source": [
    "sentencepiece_model_pb2.SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SentencePieceProcessor.GetPieceSize of <sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f51c809f4b0> >>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_piec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/karypisg/romer333/anaconda3/envs/playground-augmented-llm/lib/python3.11/site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground-augmented-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
